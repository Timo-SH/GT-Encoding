 46%|████▌     | 72/156 [00:16<00:19,  4.29batch/s]/u/timo.stoll/.bashrc: line 1: /work/log1/timo.stoll/conda/etc/profile.d/conda.sh: Permission denied
 47%|████▋     | 73/156 [00:16<00:17,  4.67batch/s]/u/timo.stoll/.bashrc: line 18: user: No such file or directory
 47%|████▋     | 74/156 [00:17<00:16,  5.00batch/s] 48%|████▊     | 75/156 [00:17<00:15,  5.25batch/s] 49%|████▊     | 76/156 [00:17<00:14,  5.40batch/s] 49%|████▉     | 77/156 [00:17<00:14,  5.53batch/s] 50%|█████     | 78/156 [00:17<00:13,  5.60batch/s] 51%|█████     | 79/156 [00:17<00:13,  5.64batch/s] 51%|█████▏    | 80/156 [00:18<00:22,  3.40batch/s] 52%|█████▏    | 81/156 [00:18<00:19,  3.90batch/s] 53%|█████▎    | 82/156 [00:18<00:17,  4.31batch/s] 53%|█████▎    | 83/156 [00:19<00:15,  4.66batch/s] 54%|█████▍    | 84/156 [00:19<00:14,  5.00batch/s] 54%|█████▍    | 85/156 [00:19<00:13,  5.24batch/s] 55%|█████▌    | 86/156 [00:19<00:12,  5.41batch/s] 56%|█████▌    | 87/156 [00:19<00:12,  5.53batch/s] 56%|█████▋    | 88/156 [00:19<00:12,  5.63batch/s] 57%|█████▋    | 89/156 [00:20<00:11,  5.75batch/s]/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' via  'pip install pytorch_lightning' in order to use GraphGym
  warnings.warn("Please install 'pytorch_lightning' via  "
 58%|█████▊    | 90/156 [00:20<00:19,  3.43batch/s] 58%|█████▊    | 91/156 [00:20<00:16,  3.93batch/s] 59%|█████▉    | 92/156 [00:21<00:14,  4.27batch/s] 60%|█████▉    | 93/156 [00:21<00:13,  4.66batch/s] 60%|██████    | 94/156 [00:21<00:12,  4.97batch/s] 61%|██████    | 95/156 [00:21<00:11,  5.18batch/s] 62%|██████▏   | 96/156 [00:21<00:11,  5.23batch/s] 62%|██████▏   | 97/156 [00:21<00:10,  5.41batch/s] 63%|██████▎   | 98/156 [00:22<00:10,  5.56batch/s] 63%|██████▎   | 99/156 [00:22<00:09,  5.72batch/s] 64%|██████▍   | 100/156 [00:22<00:16,  3.38batch/s]wandb: Currently logged in as: timo_s (timo_s-rwth-aachen-university). Use `wandb login --relogin` to force relogin
 65%|██████▍   | 101/156 [00:22<00:14,  3.88batch/s] 65%|██████▌   | 102/156 [00:23<00:12,  4.27batch/s] 66%|██████▌   | 103/156 [00:23<00:11,  4.69batch/s] 67%|██████▋   | 104/156 [00:23<00:10,  5.03batch/s] 67%|██████▋   | 105/156 [00:23<00:09,  5.22batch/s] 68%|██████▊   | 106/156 [00:23<00:09,  5.42batch/s] 69%|██████▊   | 107/156 [00:23<00:08,  5.56batch/s] 69%|██████▉   | 108/156 [00:24<00:08,  5.60batch/s] 70%|██████▉   | 109/156 [00:24<00:08,  5.69batch/s] 71%|███████   | 110/156 [00:24<00:14,  3.27batch/s] 71%|███████   | 111/156 [00:25<00:12,  3.74batch/s] 72%|███████▏  | 112/156 [00:25<00:10,  4.19batch/s]wandb: wandb version 0.18.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /work/log1/timo.stoll/TransformerExp/TransformerExp/wandb/run-20241010_141832-2h6yd1eu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-tree-590
wandb: ⭐️ View project at https://wandb.ai/timo_s-rwth-aachen-university/SAN_ZINC
wandb: 🚀 View run at https://wandb.ai/timo_s-rwth-aachen-university/SAN_ZINC/runs/2h6yd1eu
 72%|███████▏  | 113/156 [00:25<00:09,  4.55batch/s]/work/log1/timo.stoll/TransformerExp/TransformerExp/alchemy_main.py:102: UserWarning: You have chosen to seed training with CUDNN deterministic setting,which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training with CUDNN deterministic setting,'
/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):
/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):
/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch_geometric/io/fs.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(f, map_location)
 73%|███████▎  | 114/156 [00:25<00:08,  4.88batch/s] 74%|███████▎  | 115/156 [00:25<00:07,  5.14batch/s] 74%|███████▍  | 116/156 [00:25<00:07,  5.37batch/s] 75%|███████▌  | 117/156 [00:26<00:07,  5.45batch/s] 76%|███████▌  | 118/156 [00:26<00:07,  5.19batch/s] 76%|███████▋  | 119/156 [00:26<00:06,  5.36batch/s] 77%|███████▋  | 120/156 [00:27<00:11,  3.03batch/s] 78%|███████▊  | 121/156 [00:27<00:09,  3.52batch/s] 78%|███████▊  | 122/156 [00:27<00:08,  4.01batch/s] 79%|███████▉  | 123/156 [00:27<00:07,  4.39batch/s] 79%|███████▉  | 124/156 [00:27<00:06,  4.75batch/s] 80%|████████  | 125/156 [00:28<00:06,  5.02batch/s] 81%|████████  | 126/156 [00:28<00:05,  5.21batch/s] 81%|████████▏ | 127/156 [00:28<00:05,  5.33batch/s] 82%|████████▏ | 128/156 [00:28<00:05,  5.51batch/s] 83%|████████▎ | 129/156 [00:28<00:04,  5.62batch/s]/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
/work/log1/timo.stoll/TransformerExp/TransformerExp/train.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
  0%|          | 0/156 [00:00<?, ?batch/s]  0%|          | 0/156 [00:00<?, ?batch/s]
Traceback (most recent call last):
  File "/work/log1/timo.stoll/TransformerExp/TransformerExp/alchemy_main.py", line 124, in <module>
    run(config=configuration, create_dataset=loader_alchemy, create_model=create_GT, train=train, test=test)
  File "/work/log1/timo.stoll/TransformerExp/TransformerExp/alchemy_main.py", line 60, in run
    train_loss = train(train_loader, model, optimizer,loss, config.device)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/log1/timo.stoll/TransformerExp/TransformerExp/train.py", line 35, in train
    for data in tepoch:
  File "/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/work/log1/timo.stoll/TransformerExp/TransformerExp/data/Dataloader.py", line 111, in __call__
    raise TypeError(f'DataLoader found invalid type: {type(elem)}')
TypeError: DataLoader found invalid type: <class 'NoneType'>
 83%|████████▎ | 130/156 [00:29<00:08,  3.09batch/s] 84%|████████▍ | 131/156 [00:29<00:06,  3.60batch/s] 85%|████████▍ | 132/156 [00:29<00:05,  4.09batch/s] 85%|████████▌ | 133/156 [00:29<00:05,  4.44batch/s] 86%|████████▌ | 134/156 [00:30<00:04,  4.68batch/s] 87%|████████▋ | 135/156 [00:30<00:04,  4.93batch/s] 87%|████████▋ | 136/156 [00:30<00:04,  4.99batch/s] 88%|████████▊ | 137/156 [00:30<00:03,  5.09batch/s] 88%|████████▊ | 138/156 [00:30<00:03,  5.36batch/s] 89%|████████▉ | 139/156 [00:31<00:03,  5.34batch/s]wandb: - 0.012 MB of 0.012 MB uploaded 90%|████████▉ | 140/156 [00:31<00:05,  3.10batch/s] 90%|█████████ | 141/156 [00:31<00:04,  3.61batch/s] 91%|█████████ | 142/156 [00:32<00:03,  4.10batch/s]wandb: \ 0.012 MB of 0.012 MB uploaded 92%|█████████▏| 143/156 [00:32<00:02,  4.40batch/s] 92%|█████████▏| 144/156 [00:32<00:02,  4.64batch/s] 93%|█████████▎| 145/156 [00:32<00:02,  5.00batch/s] 94%|█████████▎| 146/156 [00:32<00:01,  5.09batch/s] 94%|█████████▍| 147/156 [00:32<00:01,  5.16batch/s] 95%|█████████▍| 148/156 [00:33<00:01,  5.39batch/s]wandb: | 0.012 MB of 0.012 MB uploaded 96%|█████████▌| 149/156 [00:33<00:01,  5.35batch/s] 96%|█████████▌| 150/156 [00:33<00:01,  3.09batch/s]wandb: / 0.012 MB of 0.012 MB uploaded 97%|█████████▋| 151/156 [00:34<00:01,  3.51batch/s] 97%|█████████▋| 152/156 [00:34<00:01,  3.93batch/s] 98%|█████████▊| 153/156 [00:34<00:00,  4.28batch/s] 99%|█████████▊| 154/156 [00:34<00:00,  4.58batch/s] 99%|█████████▉| 155/156 [00:34<00:00,  4.90batch/s]100%|██████████| 156/156 [00:35<00:00,  5.21batch/s]100%|██████████| 156/156 [00:35<00:00,  4.46batch/s]
wandb: - 0.012 MB of 0.012 MB uploadedwandb: \ 0.014 MB of 0.021 MB uploadedwandb: | 0.021 MB of 0.026 MB uploadedwandb: 🚀 View run proud-tree-590 at: https://wandb.ai/timo_s-rwth-aachen-university/SAN_ZINC/runs/2h6yd1eu
wandb: ⭐️ View project at: https://wandb.ai/timo_s-rwth-aachen-university/SAN_ZINC
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241010_141832-2h6yd1eu/logs
/work/log1/timo.stoll/conda/envs/GTencoding/lib/python3.12/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/work/log1/timo.stoll/TransformerExp/TransformerExp/train.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
  0%|          | 0/156 [00:00<?, ?batch/s]  1%|          | 1/156 [00:00<00:35,  4.32batch/s]  1%|▏         | 2/156 [00:00<00:40,  3.81batch/s]  2%|▏         | 3/156 [00:00<00:34,  4.39batch/s]  3%|▎         | 4/156 [00:01<00:58,  2.61batch/s]  3%|▎         | 5/156 [00:01<00:47,  3.21batch/s]  4%|▍         | 6/156 [00:01<00:40,  3.72batch/s]  4%|▍         | 7/156 [00:01<00:34,  4.30batch/s]  5%|▌         | 8/156 [00:02<00:30,  4.79batch/s]  6%|▌         | 9/156 [00:02<00:28,  5.20batch/s]  6%|▋         | 10/156 [00:02<00:26,  5.53batch/s]  7%|▋         | 11/156 [00:02<00:25,  5.75batch/s]  8%|▊         | 12/156 [00:02<00:24,  5.85batch/s]  8%|▊         | 13/156 [00:02<00:24,  5.92batch/s]  9%|▉         | 14/156 [00:03<00:43,  3.27batch/s] 10%|▉         | 15/156 [00:03<00:37,  3.81batch/s] 10%|█         | 16/156 [00:03<00:33,  4.23batch/s] 11%|█         | 17/156 [00:03<00:29,  4.70batch/s] 12%|█▏        | 18/156 [00:04<00:27,  5.06batch/s] 12%|█▏        | 19/156 [00:04<00:25,  5.41batch/s] 13%|█▎        | 20/156 [00:04<00:25,  5.43batch/s] 13%|█▎        | 21/156 [00:04<00:24,  5.56batch/s] 14%|█▍        | 22/156 [00:04<00:23,  5.76batch/s] 15%|█▍        | 23/156 [00:04<00:22,  5.92batch/s] 15%|█▌        | 24/156 [00:05<00:38,  3.41batch/s] 16%|█▌        | 25/156 [00:05<00:33,  3.90batch/s] 17%|█▋        | 26/156 [00:05<00:29,  4.38batch/s] 17%|█▋        | 27/156 [00:05<00:26,  4.82batch/s] 18%|█▊        | 28/156 [00:06<00:24,  5.20batch/s] 19%|█▊        | 29/156 [00:06<00:23,  5.48batch/s] 19%|█▉        | 30/156 [00:06<00:22,  5.65batch/s] 20%|█▉        | 31/156 [00:06<00:21,  5.83batch/s] 21%|██        | 32/156 [00:06<00:20,  5.98batch/s] 21%|██        | 33/156 [00:06<00:20,  6.02batch/s] 22%|██▏       | 34/156 [00:07<00:37,  3.23batch/s] 22%|██▏       | 35/156 [00:07<00:32,  3.77batch/s] 23%|██▎       | 36/156 [00:07<00:28,  4.26batch/s] 24%|██▎       | 37/156 [00:08<00:25,  4.72batch/s] 24%|██▍       | 38/156 [00:08<00:23,  5.10batch/s] 25%|██▌       | 39/156 [00:08<00:21,  5.43batch/s] 26%|██▌       | 40/156 [00:08<00:20,  5.70batch/s] 26%|██▋       | 41/156 [00:08<00:19,  5.87batch/s] 27%|██▋       | 42/156 [00:08<00:18,  6.00batch/s] 28%|██▊       | 43/156 [00:09<00:18,  6.09batch/s] 28%|██▊       | 44/156 [00:09<00:32,  3.44batch/s] 29%|██▉       | 45/156 [00:09<00:28,  3.93batch/s] 29%|██▉       | 46/156 [00:09<00:25,  4.38batch/s] 30%|███       | 47/156 [00:10<00:22,  4.85batch/s] 31%|███       | 48/156 [00:10<00:21,  5.12batch/s] 31%|███▏      | 49/156 [00:10<00:20,  5.25batch/s] 32%|███▏      | 50/156 [00:10<00:19,  5.56batch/s] 33%|███▎      | 51/156 [00:10<00:18,  5.66batch/s] 33%|███▎      | 52/156 [00:10<00:17,  5.83batch/s] 34%|███▍      | 53/156 [00:11<00:17,  6.00batch/s] 35%|███▍      | 54/156 [00:11<00:28,  3.59batch/s] 35%|███▌      | 55/156 [00:11<00:24,  4.13batch/s] 36%|███▌      | 56/156 [00:11<00:21,  4.61batch/s] 37%|███▋      | 57/156 [00:12<00:19,  5.01batch/s] 37%|███▋      | 58/156 [00:12<00:18,  5.35batch/s] 38%|███▊      | 59/156 [00:12<00:17,  5.62batch/s] 38%|███▊      | 60/156 [00:12<00:16,  5.82batch/s] 39%|███▉      | 61/156 [00:12<00:15,  6.00batch/s] 40%|███▉      | 62/156 [00:12<00:15,  6.07batch/s] 40%|████      | 63/156 [00:13<00:15,  6.14batch/s] 41%|████      | 64/156 [00:13<00:25,  3.59batch/s] 42%|████▏     | 65/156 [00:13<00:22,  4.03batch/s] 42%|████▏     | 66/156 [00:13<00:20,  4.34batch/s] 43%|████▎     | 67/156 [00:14<00:18,  4.76batch/s] 44%|████▎     | 68/156 [00:14<00:17,  4.98batch/s] 44%|████▍     | 69/156 [00:14<00:16,  5.33batch/s] 45%|████▍     | 70/156 [00:14<00:15,  5.40batch/s] 46%|████▌     | 71/156 [00:14<00:15,  5.56batch/s] 46%|████▌     | 72/156 [00:14<00:14,  5.77batch/s] 47%|████▋     | 73/156 [00:15<00:14,  5.79batch/s] 47%|████▋     | 74/156 [00:15<00:25,  3.16batch/s] 48%|████▊     | 75/156 [00:15<00:21,  3.70batch/s] 49%|████▊     | 76/156 [00:16<00:19,  4.12batch/s] 49%|████▉     | 77/156 [00:16<00:17,  4.61batch/s] 50%|█████     | 78/156 [00:16<00:15,  4.99batch/s] 51%|█████     | 79/156 [00:16<00:14,  5.34batch/s] 51%|█████▏    | 80/156 [00:16<00:13,  5.61batch/s] 52%|█████▏    | 81/156 [00:16<00:12,  5.78batch/s] 53%|█████▎    | 82/156 [00:17<00:12,  5.96batch/s] 53%|█████▎    | 83/156 [00:17<00:12,  5.96batch/s] 54%|█████▍    | 84/156 [00:17<00:20,  3.54batch/s] 54%|█████▍    | 85/156 [00:17<00:17,  4.08batch/s] 55%|█████▌    | 86/156 [00:18<00:15,  4.57batch/s] 56%|█████▌    | 87/156 [00:18<00:14,  4.82batch/s] 56%|█████▋    | 88/156 [00:18<00:13,  5.20batch/s] 57%|█████▋    | 89/156 [00:18<00:12,  5.34batch/s] 58%|█████▊    | 90/156 [00:18<00:12,  5.40batch/s] 58%|█████▊    | 91/156 [00:18<00:11,  5.60batch/s] 59%|█████▉    | 92/156 [00:19<00:11,  5.60batch/s] 60%|█████▉    | 93/156 [00:19<00:10,  5.80batch/s] 60%|██████    | 94/156 [00:19<00:17,  3.50batch/s] 61%|██████    | 95/156 [00:20<00:15,  3.89batch/s] 62%|██████▏   | 96/156 [00:20<00:13,  4.38batch/s] 62%|██████▏   | 97/156 [00:20<00:12,  4.66batch/s] 63%|██████▎   | 98/156 [00:20<00:11,  5.05batch/s] 63%|██████▎   | 99/156 [00:20<00:10,  5.34batch/s] 64%|██████▍   | 100/156 [00:20<00:10,  5.58batch/s] 65%|██████▍   | 101/156 [00:21<00:09,  5.77batch/s] 65%|██████▌   | 102/156 [00:21<00:09,  5.92batch/s] 66%|██████▌   | 103/156 [00:21<00:08,  6.05batch/s] 67%|██████▋   | 104/156 [00:21<00:14,  3.52batch/s] 67%|██████▋   | 105/156 [00:22<00:12,  4.05batch/s] 68%|██████▊   | 106/156 [00:22<00:11,  4.54batch/s] 69%|██████▊   | 107/156 [00:22<00:09,  4.93batch/s] 69%|██████▉   | 108/156 [00:22<00:09,  5.30batch/s] 70%|██████▉   | 109/156 [00:22<00:08,  5.58batch/s] 71%|███████   | 110/156 [00:22<00:08,  5.72batch/s] 71%|███████   | 111/156 [00:23<00:07,  5.86batch/s] 72%|███████▏  | 112/156 [00:23<00:07,  6.01batch/s] 72%|███████▏  | 113/156 [00:23<00:07,  6.10batch/s] 73%|███████▎  | 114/156 [00:23<00:12,  3.24batch/s] 74%|███████▎  | 115/156 [00:24<00:10,  3.76batch/s] 74%|███████▍  | 116/156 [00:24<00:09,  4.29batch/s] 75%|███████▌  | 117/156 [00:24<00:08,  4.76batch/s] 76%|███████▌  | 118/156 [00:24<00:07,  5.12batch/s] 76%|███████▋  | 119/156 [00:24<00:06,  5.43batch/s] 77%|███████▋  | 120/156 [00:24<00:06,  5.61batch/s] 78%|███████▊  | 121/156 [00:25<00:06,  5.81batch/s] 78%|███████▊  | 122/156 [00:25<00:05,  5.96batch/s] 79%|███████▉  | 123/156 [00:25<00:05,  6.03batch/s] 79%|███████▉  | 124/156 [00:26<00:09,  3.35batch/s] 80%|████████  | 125/156 [00:26<00:07,  3.90batch/s] 81%|████████  | 126/156 [00:26<00:06,  4.40batch/s] 81%|████████▏ | 127/156 [00:26<00:05,  4.86batch/s] 82%|████████▏ | 128/156 [00:26<00:05,  5.14batch/s] 83%|████████▎ | 129/156 [00:26<00:04,  5.45batch/s] 83%|████████▎ | 130/156 [00:26<00:04,  5.68batch/s] 84%|████████▍ | 131/156 [00:27<00:04,  5.87batch/s] 85%|████████▍ | 132/156 [00:27<00:04,  5.97batch/s] 85%|████████▌ | 133/156 [00:27<00:03,  6.10batch/s] 86%|████████▌ | 134/156 [00:28<00:06,  3.29batch/s] 87%|████████▋ | 135/156 [00:28<00:05,  3.82batch/s] 87%|████████▋ | 136/156 [00:28<00:04,  4.36batch/s] 88%|████████▊ | 137/156 [00:28<00:03,  4.78batch/s] 88%|████████▊ | 138/156 [00:28<00:03,  5.18batch/s] 89%|████████▉ | 139/156 [00:28<00:03,  5.44batch/s] 90%|████████▉ | 140/156 [00:29<00:02,  5.66batch/s] 90%|█████████ | 141/156 [00:29<00:02,  5.86batch/s] 91%|█████████ | 142/156 [00:29<00:02,  5.97batch/s] 92%|█████████▏| 143/156 [00:29<00:02,  6.07batch/s] 92%|█████████▏| 144/156 [00:30<00:03,  3.27batch/s] 93%|█████████▎| 145/156 [00:30<00:02,  3.85batch/s] 94%|█████████▎| 146/156 [00:30<00:02,  4.27batch/s] 94%|█████████▍| 147/156 [00:30<00:01,  4.67batch/s] 95%|█████████▍| 148/156 [00:30<00:01,  4.91batch/s] 96%|█████████▌| 149/156 [00:30<00:01,  5.26batch/s] 96%|█████████▌| 150/156 [00:31<00:01,  5.37batch/s] 97%|█████████▋| 151/156 [00:31<00:00,  5.65batch/s] 97%|█████████▋| 152/156 [00:31<00:00,  5.85batch/s] 98%|█████████▊| 153/156 [00:31<00:00,  6.03batch/s] 99%|█████████▊| 154/156 [00:32<00:00,  3.52batch/s] 99%|█████████▉| 155/156 [00:32<00:00,  4.07batch/s]100%|██████████| 156/156 [00:32<00:00,  4.55batch/s]100%|██████████| 156/156 [00:32<00:00,  4.80batch/s]
  0%|          | 0/156 [00:00<?, ?batch/s]  1%|          | 1/156 [00:00<00:40,  3.78batch/s]  1%|▏         | 2/156 [00:00<00:32,  4.73batch/s]  2%|▏         | 3/156 [00:00<00:47,  3.21batch/s]  3%|▎         | 4/156 [00:01<00:38,  3.97batch/s]  3%|▎         | 5/156 [00:01<00:33,  4.56batch/s]  4%|▍         | 6/156 [00:01<00:29,  5.07batch/s]  4%|▍         | 7/156 [00:01<00:27,  5.37batch/s]  5%|▌         | 8/156 [00:01<00:36,  4.00batch/s]  6%|▌         | 9/156 [00:02<00:32,  4.48batch/s]  6%|▋         | 10/156 [00:02<00:29,  4.91batch/s]  7%|▋         | 11/156 [00:02<00:27,  5.25batch/s]  8%|▊         | 12/156 [00:02<00:26,  5.51batch/s]  8%|▊         | 13/156 [00:02<00:34,  4.13batch/s]  9%|▉         | 14/156 [00:03<00:30,  4.59batch/s] 10%|▉         | 15/156 [00:03<00:28,  5.02batch/s] 10%|█         | 16/156 [00:03<00:26,  5.37batch/s] 11%|█         | 17/156 [00:03<00:24,  5.66batch/s] 12%|█▏        | 18/156 [00:03<00:33,  4.15batch/s] 12%|█▏        | 19/156 [00:04<00:29,  4.65batch/s] 13%|█▎        | 20/156 [00:04<00:26,  5.04batch/s] 13%|█▎        | 21/156 [00:04<00:25,  5.40batch/s] 14%|█▍        | 22/156 [00:04<00:23,  5.62batch/s] 15%|█▍        | 23/156 [00:04<00:31,  4.27batch/s]slurmstepd-cn-302: error: *** JOB 8956080 ON cn-302 CANCELLED AT 2024-10-10T14:19:27 ***
 15%|█▌        | 24/156 [00:05<00:27,  4.72batch/s]